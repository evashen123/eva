{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nimport re\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n\n\n# In[2]:\n\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "glove=pd.read_csv(\"../input/glove.840B.300d.txt\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "test.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(train.shape)\nprint(test.shape)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train = train.fillna('empty')\ntest = test.fillna('empty')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(train.isnull().sum())\nprint(test.isnull().sum())",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "for i in range(6):\n    print(train.question1[i])\n    print(train.question2[i])\n    print()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n    # Clean the text, with the option to remove stopwords and to stem words.\n    \n    # Convert words to lower case and split them\n    text = text.lower().split()\n\n    # Optionally remove stop words (true by default)\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        text = [w for w in text if not w in stops]\n    \n    text = \" \".join(text)\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\'+-=]\", \" \", text)\n    text = re.sub(r\"\\'s\", \" 's \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \" cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    \n    # Shorten words to their stems\n    if stem_words:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stemmed_words = [stemmer.stem(word) for word in text]\n        text = \" \".join(stemmed_words)\n    \n    # Return a list of words\n    return(text)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def process_questions(question_list, questions, question_list_name, dataframe):\n# function to transform questions and display progress\n    for question in questions:\n        question_list.append(text_to_wordlist(question))\n        if len(question_list) % 100000 == 0:\n            progress = len(question_list)/len(dataframe) * 100\n            print(\"{} is {}% complete.\".format(question_list_name, round(progress, 1)))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train_question1 = []\nprocess_questions(train_question1, train.question1, 'train_question1', train)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train_question1[:10]",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train_question2 = []\nprocess_questions(train_question2, train.question2, 'train_question2', train)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "test_question1 = []\nprocess_questions(test_question1, test.question1, 'test_question1', test)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\n\ntest_question2 = []\nprocess_questions(test_question2, test.question2, 'test_question2', test)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport datetime, time, json\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Dense, Dropout, Reshape, Merge, BatchNormalization, TimeDistributed,                          Lambda, Activation, LSTM, Flatten, Bidirectional, Convolution1D, GRU, MaxPooling1D,                          Convolution2D\nfrom keras.regularizers import l2\nfrom keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import SGD\nfrom collections import defaultdict",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "word_count = defaultdict(int)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "for question in train_question1:\n    word_count[question] += 1\nprint(\"train_question1 is complete.\")\n    \nfor question in train_question2:\n    word_count[question] += 1\nprint(\"train_question2 is complete\")\n\nfor question in test_question1:\n    word_count[question] += 1\nprint(\"test_question1 is complete.\")\n\nfor question in test_question2:\n    word_count[question] += 1\nprint(\"test_question2 is complete\")\n\nprint(\"Total number of unique words:\", len(word_count))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "lengths = []\nfor question in train_question1:\n    lengths.append(len(question.split()))\n\nfor question in train_question2:\n    lengths.append(len(question.split()))\n\n# Create a dataframe so that the values can be inspected\nlengths = pd.DataFrame(lengths, columns=['counts'])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "lengths.counts.describe()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "np.percentile(lengths.counts, 99.5)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "num_words = 200000\n\ntrain_questions = train_question1 + train_question2\ntokenizer = Tokenizer(nb_words = num_words)\ntokenizer.fit_on_texts(train_questions)\nprint(\"Fitting is compelte.\")\ntrain_question1_word_sequences = tokenizer.texts_to_sequences(train_question1)\nprint(\"train_question1 is complete.\")\ntrain_question2_word_sequences = tokenizer.texts_to_sequences(train_question2)\nprint(\"train_question2 is complete\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "test_question1_word_sequences = tokenizer.texts_to_sequences(test_question1)\nprint(\"test_question1 is complete.\")\ntest_question2_word_sequences = tokenizer.texts_to_sequences(test_question2)\nprint(\"test_question2 is complete.\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "word_index = tokenizer.word_index\nprint(\"Words in index: %d\" % len(word_index))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "max_question_len = 37\n\ntrain_q1 = pad_sequences(train_question1_word_sequences, \n                              maxlen = max_question_len,\n                              padding = 'post',\n                              truncating = 'post')\nprint(\"train_q1 is complete.\")\n\ntrain_q2 = pad_sequences(train_question2_word_sequences, \n                              maxlen = max_question_len,\n                              padding = 'post',\n                              truncating = 'post')\nprint(\"train_q2 is complete.\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "test_q1 = pad_sequences(test_question1_word_sequences, \n                             maxlen = max_question_len,\n                             padding = 'post',\n                             truncating = 'post')\nprint(\"test_q1 is complete.\")\n\ntest_q2 = pad_sequences(test_question2_word_sequences, \n                             maxlen = max_question_len,\n                             padding = 'post',\n                             truncating = 'post')\nprint(\"test_q2 is complete.\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "y_train = train.is_duplicate",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "embeddings_index = {}\nwith open('glove.840B.300d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split(' ')\n        word = values[0]\n        embedding = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = embedding\n\nprint('Word embeddings:', len(embeddings_index))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\n\n\n# Load GloVe to use pretrained vectors\n# From this link: https://nlp.stanford.edu/projects/glove/\n\n\n# In[50]:\n\n# Need to use 300 for embedding dimensions to match GloVe vectors.\nembedding_dim = 300\n\nnb_words = len(word_index)\nword_embedding_matrix = np.zeros((nb_words + 1, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        word_embedding_matrix[i] = embedding_vector\n\nprint('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))\n\n\n# In[66]:\n\nunits = 150\ndropout = 0.25\nnb_filter = 32\nfilter_length = 3\nembedding_dim = 300\n\nmodel1 = Sequential()\nmodel1.add(Embedding(nb_words + 1,\n                     embedding_dim,\n                     weights = [word_embedding_matrix],\n                     input_length = max_question_len,\n                     trainable = False))\n\nmodel1.add(Convolution1D(nb_filter = nb_filter, \n                        filter_length = filter_length, \n                        border_mode = 'same'))\nmodel1.add(BatchNormalization())\nmodel1.add(Activation('relu'))\nmodel1.add(Dropout(dropout))\n\nmodel1.add(Convolution1D(nb_filter = nb_filter, \n                        filter_length = filter_length, \n                        border_mode = 'same'))\nmodel1.add(BatchNormalization())\nmodel1.add(Activation('relu'))\nmodel1.add(Dropout(dropout))\n\nmodel1.add(Flatten())\n\n\n\nmodel2 = Sequential()\nmodel2.add(Embedding(nb_words + 1,\n                     embedding_dim,\n                     weights = [word_embedding_matrix],\n                     input_length = max_question_len,\n                     trainable = False))\n\nmodel2.add(Convolution1D(nb_filter = nb_filter, \n                        filter_length = filter_length, \n                        border_mode = 'same'))\nmodel2.add(BatchNormalization())\nmodel2.add(Activation('relu'))\nmodel2.add(Dropout(dropout))\n\nmodel2.add(Convolution1D(nb_filter = nb_filter, \n                        filter_length = filter_length, \n                        border_mode = 'same'))\nmodel2.add(BatchNormalization())\nmodel2.add(Activation('relu'))\nmodel2.add(Dropout(dropout))\n\nmodel2.add(Flatten())\n\n\n\nmodel3 = Sequential()\nmodel3.add(Embedding(nb_words + 1,\n                     embedding_dim,\n                     weights = [word_embedding_matrix],\n                     input_length = max_question_len,\n                     trainable = False))\nmodel3.add(TimeDistributed(Dense(embedding_dim)))\nmodel3.add(BatchNormalization())\nmodel3.add(Activation('relu'))\nmodel3.add(Dropout(dropout))\nmodel3.add(Lambda(lambda x: K.max(x, axis=1), output_shape=(embedding_dim, )))\n\n\nmodel4 = Sequential()\nmodel4.add(Embedding(nb_words + 1,\n                     embedding_dim,\n                     weights = [word_embedding_matrix],\n                     input_length = max_question_len,\n                     trainable = False))\nmodel4.add(TimeDistributed(Dense(embedding_dim)))\nmodel4.add(BatchNormalization())\nmodel4.add(Activation('relu'))\nmodel4.add(Dropout(dropout))\nmodel4.add(Lambda(lambda x: K.max(x, axis=1), output_shape=(embedding_dim, )))\n\n\nmodela = Sequential()\nmodela.add(Merge([model1, model2], mode='concat'))\nmodela.add(Dense(units))\nmodela.add(BatchNormalization())\nmodela.add(Activation('relu'))\nmodela.add(Dropout(dropout))\n\nmodela.add(Dense(units))\nmodela.add(BatchNormalization())\nmodela.add(Activation('relu'))\nmodela.add(Dropout(dropout))\n\n\nmodelb = Sequential()\nmodelb.add(Merge([model3, model4], mode='concat'))\nmodelb.add(Dense(units))\nmodelb.add(BatchNormalization())\nmodelb.add(Activation('relu'))\nmodelb.add(Dropout(dropout))\n\nmodelb.add(Dense(units))\nmodelb.add(BatchNormalization())\nmodelb.add(Activation('relu'))\nmodelb.add(Dropout(dropout))\n\n\nmodel = Sequential()\nmodel.add(Merge([modela, modelb], mode='concat'))\nmodel.add(Dense(units))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(dropout))\n\nmodel.add(Dense(units))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(dropout))\n\nmodel.add(Dense(1))\nmodel.add(BatchNormalization())\nmodel.add(Activation('sigmoid'))\n#sgd = SGD(lr=0.01, decay=5e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n\n# In[67]:\n\nsave_best_weights = 'question_pairs_weights.h5'\n\nt0 = time.time()\ncallbacks = [ModelCheckpoint(save_best_weights, monitor='val_loss', save_best_only=True),\n             EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')]\nhistory = model.fit([train_q1, train_q2],\n                    y_train,\n                    batch_size=200,\n                    nb_epoch=100,\n                    validation_split=0.1,\n                    verbose=True,\n                    shuffle=True,\n                    callbacks=callbacks)\nt1 = time.time()\nprint(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))\n\n\n\n# In[68]:\n\nsummary_stats = pd.DataFrame({'epoch': [ i + 1 for i in history.epoch ],\n                              'train_acc': history.history['acc'],\n                              'valid_acc': history.history['val_acc'],\n                              'train_loss': history.history['loss'],\n                              'valid_loss': history.history['val_loss']})\n\n\n# In[69]:\n\nsummary_stats\n\n\n# In[70]:\n\nplt.plot(summary_stats.train_loss)\nplt.plot(summary_stats.valid_loss)\nplt.show()\n\n\n# In[71]:\n\nmin_loss, idx = min((loss, idx) for (idx, loss) in enumerate(history.history['val_loss']))\nprint('Minimum loss at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(min_loss))\nmin_loss = round(min_loss, 4)\n\n\n# In[72]:\n\nmodel.load_weights(save_best_weights)\npredictions = model.predict([test_q1, test_q2], verbose = True)\n\n\n# In[73]:\n\n#Create submission\nsubmission = pd.DataFrame(predictions, columns=['is_duplicate'])\nsubmission.insert(0, 'test_id', test.test_id)\nfile_name = 'submission_{}.csv'.format(min_loss)\nsubmission.to_csv(file_name, index=False)\n\n\n# In[74]:\n\nsubmission.head(10)\n",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}